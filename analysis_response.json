{"resume_classification":{"primary_classification":"Resume","confidence":0.99,"reasoning":"The document contains a summary of the candidate's skills and experiences, educational background, work history, projects, and technical skills. This structure is consistent with a standard resume format. The information is presented in a way that highlights accomplishments and quantifiable results, typical of a resume designed to showcase a candidate's qualifications for a job.","file_type":"application/pdf","llm_model_used":"gemini-1.5-flash"},"resume_entities":{"entities":{"companies":["SAP LABS","Arizona State University"],"dates":["Jan. 2024 – Dec. 2025","Aug. 2017 – May 2021","Jul. 2021 – Dec. 2023","Jan. 2025 – Present","Feb. 2021 – Jul. 2021","Jun. 2024 – Aug. 2024","Jan. 2025"],"skills":["distributed backend systems","cloud solutions","ML-driven automation","system design","performance tuning","real-world ML applications","microservices","purchase order","API performance","order processing","legacy applications","SAP Cloud","scalability","Redis caching","user engagement","bug prediction","knowledge distillation","LLM compression","inference latency","LayerDrop","pruning techniques","compressed LLMs","Jira workflow","JavaScript","HTML","full-stack features","React","MongoDB","AWS","Python","YOLOv8","OpenCV","KMeans","Optical Flow","player/ball detection","KMeans clustering","team identification","player movements","perspective transformation","LangChain","Hugging Face","ChromaDB","Vector Search","Retrieval-Augmented Generation","RAG","real-time data","LLMs","response accuracy","data chunking","Flask API","NLP","Computer Vision","LLMs","Recommendation Systems","Docker","Kubernetes","Kafka","Elastic Search","Redis","Git","REST APIs","Microservices","Distributed Systems","Agile","CI/CD","C/C++","Java","SQL","HTML/CSS"],"job_titles":["Software Engineer","Associate Software Developer","Research Assistant","Software Development Intern"],"technologies":["YOLOv8","OpenCV","KMeans","Optical Flow","LangChain","Hugging Face","ChromaDB","Vector Search","Flask API","Redis","React","MongoDB","AWS","SAP Cloud Platform","PostgreSQL","Docker","Kubernetes","Kafka","Elastic Search","Git"],"education_degrees":["Master’s in Computer Science","Bachelor’s in Computer Science"],"universities":["Arizona State University","RV College of Engineering"],"achievements":["Improved API performance by 30%","Built microservices that cut order processing time by 90%","Reduced processing time by 90% for 5K+ monthly orders","Cut infrastructure costs by 20% and improving scalability","Optimized API response time by 30% using Redis caching, improving overall user engagement by 20%","Led development of a bug prediction tool with a 4-member team, reducing support tickets by 25%","Reduced inference latency by 45% using LayerDrop and pruning techniques","Developed a Jira workflow gadget using JavaScript and HTML, used by 10+ teams to simplify ticket tracking","Built a real-time player/ball detection system using YOLOv8, achieving 95% accuracy on Bundesliga datasets","Optimized response accuracy by implementing vector search and data chunking in ChromaDB"],"requirements":[]},"summary":{"total_extracted_entities":106,"entity_counts":{"companies":2,"dates":7,"skills":69,"job_titles":4,"technologies":20,"education_degrees":2,"universities":2},"has_contact_info":false,"technical_skills_found":true,"achievements_found":true,"requirements_found":false},"document_classification":"Resume","llm_model_used":"gemini-1.5-flash"},"jd_classification":{"primary_classification":"Job Description","confidence":0.95,"reasoning":"The text clearly describes a job opening at Microsoft's Azure Data engineering team for a Software Engineer II position. It includes responsibilities, required and preferred qualifications, and details about the team and the work.  The structure and content strongly align with the format of a typical job description.","file_type":"text","llm_model_used":"gemini-1.5-flash"},"jd_entities":{"entities":{"companies":["Microsoft"],"dates":[],"skills":["coding","C","C++","C#","Java","JavaScript","Python","distributed systems","database systems","System Services","Cloud service offering","data analytics"],"job_titles":["Software Engineer II","Software Engineer II - Infrastructure"],"technologies":["Microsoft Fabric","Azure SQL DB","Azure Cosmos DB","Azure PostgreSQL","Azure Data Factory","Azure Synapse Analytics","Azure Service Bus","Azure Event Grid","Power BI","Kubernetes","Service Fabric","Windows Fabric"],"education_degrees":["Bachelor's degree in computer science"],"universities":[],"achievements":[],"requirements":["Bachelor's degree in computer science, or related technical discipline AND 2+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python","2+ years of experience with distributed systems or database systems or System Services","Ability to meet Microsoft, customer and/or government security screening requirements","Microsoft Cloud Background Check"]},"summary":{"total_extracted_entities":28,"entity_counts":{"companies":1,"dates":0,"skills":12,"job_titles":2,"technologies":12,"education_degrees":1,"universities":0},"has_contact_info":false,"technical_skills_found":true,"achievements_found":false,"requirements_found":true},"document_classification":"Job Description","llm_model_used":"gemini-1.5-flash"},"relationship_map":{"relationship_map":{"matched_skills":[{"resume_skill":"distributed backend systems","jd_requirement":"2+ years of experience with distributed systems","confidence":1.0,"reasoning":"The candidate explicitly states 2.5+ years of experience building distributed backend systems, directly matching the JD's requirement for distributed systems experience."},{"resume_skill":"Python","jd_requirement":"coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python","confidence":1.0,"reasoning":"The candidate lists Python as a skill and demonstrates its use in multiple projects, fulfilling the JD's requirement for coding experience in Python (or other languages)."},{"resume_skill":"Java","jd_requirement":"coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python","confidence":1.0,"reasoning":"Candidate lists Java (Intermediate) under Technical Skills."},{"resume_skill":"JavaScript","jd_requirement":"coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python","confidence":1.0,"reasoning":"Candidate lists JavaScript under Technical Skills and provides evidence of its use in a project: \"Developed a Jira workflow gadget using JavaScript and HTML...\""},{"resume_skill":"C/C++","jd_requirement":"coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python","confidence":1.0,"reasoning":"Candidate lists C/C++ under Technical Skills."},{"resume_skill":"database systems","jd_requirement":"2+ years of experience with distributed systems or database systems or System Services","confidence":0.9,"reasoning":"The candidate's experience with \"Migrated legacy applications to SAP Cloud\" and building microservices for SAP ERP strongly suggests experience with database systems, even if not explicitly stated. The candidate also lists PostgreSQL and MongoDB under Technical Skills."},{"resume_skill":"System Services","jd_requirement":"2+ years of experience with distributed systems or database systems or System Services","confidence":0.7,"reasoning":"The candidate's work on API performance optimization, microservices development, and cloud migration suggests experience with system services related to backend infrastructure and cloud platforms."}],"matched_experience_to_responsibilities":[{"resume_experience_summary":"Built a purchase order microservice for SAP ERP to automate manual workflows, reducing processing time by 90% for 5K+ monthly orders.","jd_responsibility":"Own project efforts, be end-to-end accountable for a global cloud delivery.","confidence":0.9,"reasoning":"Building and deploying a microservice demonstrates ownership and end-to-end accountability, although the scale mentioned is not explicitly global."},{"resume_experience_summary":"Migrated legacy applications to SAP Cloud, cutting infrastructure costs by 20% and improving scalability.","jd_responsibility":"Work on a distributed system that manages cluster resources and maintains availability/reliability.","confidence":0.8,"reasoning":"Cloud migration involves managing resources, ensuring availability, and improving scalability, which are key aspects of working with distributed systems in a cloud environment."},{"resume_experience_summary":"Optimized API response time by 30% using Redis caching, improving overall user engagement by 20%.","jd_responsibility":"Support customers.","confidence":0.7,"reasoning":"Improving API performance and user engagement indirectly supports customers by enhancing the functionality and usability of the system."}],"identified_gaps_in_resume":[],"strong_points_in_resume":["2.5+ years of experience building distributed backend systems and deploying scalable cloud solutions.","Improved API performance by 30% and built microservices that cut order processing time by 90%.","Led development of a bug prediction tool with a 4-member team, reducing support tickets by 25%.","Reduced inference latency by 45% using LayerDrop and pruning techniques."]},"resume_id":"15d77f04-e0f4-4b83-a921-55f9af071fb2","jd_id":"jd-text-c7612fe3-8b97-4ee3-a07f-1d1f3d6fd3a2","llm_model_used":"gemini-1.5-pro"},"job_match_analysis":{"match_analysis":{"match_percentage":85,"strength_summary":"The candidate demonstrates strong alignment with the job description, possessing extensive experience in distributed systems, backend development, and database technologies.  Their proven ability to deliver impactful results, such as reducing processing times by 90% and improving API response times by 30%, showcases their practical skills and problem-solving capabilities. Proficiency in multiple programming languages (Python, Java, JavaScript, C/C++) further strengthens their candidacy. The candidate's experience with cloud migration and microservices development also aligns well with the role's focus on cloud-based distributed systems. While lacking explicit experience with Microsoft's specific technologies (like Service Fabric), their transferable skills and demonstrable achievements make them a highly competitive candidate.","areas_for_improvement":["While the candidate possesses experience with database systems, explicitly highlighting experience with large-scale, mission-critical database systems (TB+ scale) would further strengthen their application.  Demonstrating familiarity with cloud orchestration tools like Kubernetes or Service Fabric would also be beneficial.  Finally, quantifying experience in supporting customers would add further weight to their application."]},"overall_match_percentage":85,"llm_model_used":"gemini-1.5-flash"},"overall_match_percentage":85,"resume content":"Veeresh Koliwad\n602-451-5421 | veereshkoliwad99@gmail.com | linkedin.com/in/veereshkoliwad | github.com/veereshgit99\nSummary\nSoftware Engineer with 2.5+ years of experience building distributed backend systems and deploying scalable cloud\nsolutions. Led ML-driven automation, improved API performance by 30%, and built microservices that cut order\nprocessing time by 90%. Passionate about system design, performance tuning, and real-world ML applications.\nEducation\nArizona State University\nTempe, AZ\nMaster’s in Computer Science\nJan. 2024 – Dec. 2025\nRV College of Engineering\nBangalore, India\nBachelor’s in Computer Science\nAug. 2017 – May 2021\nExperience\nAssociate Software Developer\nJul. 2021 – Dec. 2023\nSAP LABS\nBangalore, India\n• Built a purchase order microservice for SAP ERP to automate manual workflows, reducing processing time by 90%\nfor 5K+ monthly orders.\n• Migrated legacy applications to SAP Cloud, cutting infrastructure costs by 20% and improving scalability.\n• Optimized API response time by 30% using Redis caching, improving overall user engagement by 20%.\n• Led development of a bug prediction tool with a 4-member team, reducing support tickets by 25%.\nResearch Assistant\nJan. 2025 – Present\nArizona State University\nTempe, AZ\n• Evaluated MiniLLM on benchmark datasets (OpenOrca, AlpacaEval) to explore trade-offs in LLM compression\nthrough knowledge distillation .\n• Reduced inference latency by 45% using LayerDrop and pruning techniques, making compressed LLMs more\npractical for deployment.\nSoftware Development Intern\nFeb. 2021 – Jul. 2021\nSAP LABS\nBangalore, India\n• Developed a Jira workflow gadget using JavaScript and HTML, used by 10+ teams to simplify ticket tracking.\n• Built and deployed full-stack features using React and MongoDB on AWS, for SAP’s Bydesign module.\nProjects\nFootball Analysis System | Python, YOLOv8, OpenCV, KMeans, Optical Flow\nJun. 2024 – Aug. 2024\n• Built a real-time player/ball detection system using YOLOv8, achieving 95% accuracy on Bundesliga datasets.\n• Implemented KMeans clustering for team identification and optical flow for tracking player movements.\n• Applied perspective transformation to standardize camera angles across match footage.\nRAG Chatbot | LangChain, Hugging Face, ChromaDB, Vector Search\nJan. 2025\n• Developed a context-aware chatbot using Retrieval-Augmented Generation (RAG) to integrate real-time data with\nLLMs.\n• Optimized response accuracy by implementing vector search and data chunking in ChromaDB.\n• Deployed the system with Flask API for seamless integration with frontend applications.\nTechnical Skills\nLanguages: Python, C/C++, Java (Intermediate), SQL, JavaScript, HTML/CSS\nMachine Learning: NLP, Computer Vision, LLMs, Recommendation Systems\nTools: Docker, Kubernetes, React, Kafka, Elastic Search, Redis, Git\nCloud & Databases: AWS, SAP Cloud Platform, PostgreSQL, MongoDB\nMethodologies: REST APIs, Microservices, Distributed Systems, Agile, CI/CD\n","user_id":"65a4c0a2-5872-4459-9a60-e7a2fadea4d0","resume_id":"15d77f04-e0f4-4b83-a921-55f9af071fb2","resume_content":"Veeresh Koliwad\n602-451-5421 | veereshkoliwad99@gmail.com | linkedin.com/in/veereshkoliwad | github.com/veereshgit99\nSummary\nSoftware Engineer with 2.5+ years of experience building distributed backend systems and deploying scalable cloud\nsolutions. Led ML-driven automation, improved API performance by 30%, and built microservices that cut order\nprocessing time by 90%. Passionate about system design, performance tuning, and real-world ML applications.\nEducation\nArizona State University\nTempe, AZ\nMaster’s in Computer Science\nJan. 2024 – Dec. 2025\nRV College of Engineering\nBangalore, India\nBachelor’s in Computer Science\nAug. 2017 – May 2021\nExperience\nAssociate Software Developer\nJul. 2021 – Dec. 2023\nSAP LABS\nBangalore, India\n• Built a purchase order microservice for SAP ERP to automate manual workflows, reducing processing time by 90%\nfor 5K+ monthly orders.\n• Migrated legacy applications to SAP Cloud, cutting infrastructure costs by 20% and improving scalability.\n• Optimized API response time by 30% using Redis caching, improving overall user engagement by 20%.\n• Led development of a bug prediction tool with a 4-member team, reducing support tickets by 25%.\nResearch Assistant\nJan. 2025 – Present\nArizona State University\nTempe, AZ\n• Evaluated MiniLLM on benchmark datasets (OpenOrca, AlpacaEval) to explore trade-offs in LLM compression\nthrough knowledge distillation .\n• Reduced inference latency by 45% using LayerDrop and pruning techniques, making compressed LLMs more\npractical for deployment.\nSoftware Development Intern\nFeb. 2021 – Jul. 2021\nSAP LABS\nBangalore, India\n• Developed a Jira workflow gadget using JavaScript and HTML, used by 10+ teams to simplify ticket tracking.\n• Built and deployed full-stack features using React and MongoDB on AWS, for SAP’s Bydesign module.\nProjects\nFootball Analysis System | Python, YOLOv8, OpenCV, KMeans, Optical Flow\nJun. 2024 – Aug. 2024\n• Built a real-time player/ball detection system using YOLOv8, achieving 95% accuracy on Bundesliga datasets.\n• Implemented KMeans clustering for team identification and optical flow for tracking player movements.\n• Applied perspective transformation to standardize camera angles across match footage.\nRAG Chatbot | LangChain, Hugging Face, ChromaDB, Vector Search\nJan. 2025\n• Developed a context-aware chatbot using Retrieval-Augmented Generation (RAG) to integrate real-time data with\nLLMs.\n• Optimized response accuracy by implementing vector search and data chunking in ChromaDB.\n• Deployed the system with Flask API for seamless integration with frontend applications.\nTechnical Skills\nLanguages: Python, C/C++, Java (Intermediate), SQL, JavaScript, HTML/CSS\nMachine Learning: NLP, Computer Vision, LLMs, Recommendation Systems\nTools: Docker, Kubernetes, React, Kafka, Elastic Search, Redis, Git\nCloud & Databases: AWS, SAP Cloud Platform, PostgreSQL, MongoDB\nMethodologies: REST APIs, Microservices, Distributed Systems, Agile, CI/CD\n","job_description":{"file_id":"jd-text-c7612fe3-8b97-4ee3-a07f-1d1f3d6fd3a2","content":"About the job\r\nMicrosoft’s Azure Data engineering team is leading the transformation of analytics in the world of data with products like databases, data integration, big data analytics, messaging & real-time analytics, and business intelligence. The products our portfolio include Microsoft Fabric, Azure SQL DB, Azure Cosmos DB, Azure PostgreSQL, Azure Data Factory, Azure Synapse Analytics, Azure Service Bus, Azure Event Grid, and Power BI. Our mission is to build the data platform for the age of AI, powering a new class of data-first applications and driving a data culture.\r\n\r\nWithin Azure Data, the databases team builds and maintains Microsoft's operational Database systems. We store and manage data in a structured way to enable multitude of applications across various industries. We are on a journey to enable developer friendly, mission-critical, AI enabled operational Databases across relational, non-relational and OSS offerings.\r\n\r\nWe are looking for a Software Engineer II to join our team.\r\n\r\nThis work focuses on cluster orchestration, usually a part of the “control plane” of cloud systems, and, is related to SQL DB’s tight integration with Microsoft’s Service Fabric (aka Windows Fabric) cloud orchestrator (the same class of system as Kubernetes). The problem at hand is broadly a distributed systems problem, with an eye towards designing and building a solution that will stand the test of time (think decade). The complexity of the component interaction graph and dependencies that are taken on the PLB component are substantial but justified due its mission critical nature.\r\n\r\nMicrosoft’s mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond.\r\n\r\nResponsibilities\r\n\r\nOwn project efforts, be end-to-end accountable for a global cloud delivery.\r\nWork on a distributed system that manages cluster resources and maintains availability/reliability.\r\nWork on multiple layers of a public cloud system's resource governance from the node level, to the geo-region level.\r\nWork with multiple component stakeholders as well as multiple product teams to achieve goals.\r\nSupport customers.\r\nEmbody our culture and values\r\n\r\nQualifications\r\n\r\nRequired Qualifications\r\n\r\nBachelor's degree in computer science, or related technical discipline AND 2+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python\r\nOR equivalent experience.\r\n2+ years of experience with distributed systems or database systems or System Services\r\nOther Requirements\r\n\r\nAbility to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check:\r\n\r\nThis position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.\r\n\r\nPreferred Qualifications\r\n\r\nExperience in Cloud service offering\r\nExperience with at-scale distributed systems, or data analytics / systems at the TB+ scale","entities":{"companies":["Microsoft"],"dates":[],"skills":["coding","C","C++","C#","Java","JavaScript","Python","distributed systems","database systems","System Services","Cloud service offering","data analytics"],"job_titles":["Software Engineer II","Software Engineer II - Infrastructure"],"technologies":["Microsoft Fabric","Azure SQL DB","Azure Cosmos DB","Azure PostgreSQL","Azure Data Factory","Azure Synapse Analytics","Azure Service Bus","Azure Event Grid","Power BI","Kubernetes","Service Fabric","Windows Fabric"],"education_degrees":["Bachelor's degree in computer science"],"universities":[],"achievements":[],"requirements":["Bachelor's degree in computer science, or related technical discipline AND 2+ years technical engineering experience with coding in languages including, but not limited to, C, C++, C#, Java, JavaScript, or Python","2+ years of experience with distributed systems or database systems or System Services","Ability to meet Microsoft, customer and/or government security screening requirements","Microsoft Cloud Background Check"]}},"resume_file_type":"application/pdf"}